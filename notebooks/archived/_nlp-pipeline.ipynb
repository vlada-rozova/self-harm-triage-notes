{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import scispacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from scispacy.linking import EntityLinker\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to process\n",
    "# filename = \"rmh_1217_test\"\n",
    "filename = \"rmh_1219\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555455, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>uid</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>arrival_mode</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>SH</th>\n",
       "      <th>SI</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1029335.0</td>\n",
       "      <td>240891</td>\n",
       "      <td>64</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>2012-01-08 00:35:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>SOB for 5/7, been to GP given prednisolone, coughing taken inhalers with minimal relief, speakin...</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sob for 5/7 , been to gp given prednisolone , coughing taken inhalers with minimal relief , spea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2073046.0</td>\n",
       "      <td>696853</td>\n",
       "      <td>31</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "      <td>2012-01-08 00:41:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>pt has lac down right forehead, to eyebrow, will require stitches and ADT, denies loc wound abou...</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pt has lac down right forehead , to eyebrow , will require stitches and adt , denies loc wound a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2073047.0</td>\n",
       "      <td>988598</td>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>road ambulance</td>\n",
       "      <td>2012-01-08 00:52:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>pt expect MBA, trapped for 45mins, #right femur, had 40mg morphine, GCS 15</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pt expect mba , trapped for 45 mins , fracture right femur , had 40 mg morphine , gcs 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1349154.0</td>\n",
       "      <td>941235</td>\n",
       "      <td>51</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "      <td>2012-01-08 01:11:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>L) sided flank pain same as previous renal colic pain unimproved with analgesia for the past 1/5...</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>left sided flank pain same as previous renal colic pain unimproved with analgesia for the past 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1367452.0</td>\n",
       "      <td>900875</td>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>other</td>\n",
       "      <td>2012-01-08 01:23:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>generalised abdo pain and associated headache for 1 year worse tonight.  Pt states that she had ...</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>generalised abdo pain and associated headache for 1 year worse tonight . pt states that she had ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id     uid  age     sex    arrival_mode         arrival_date  year  \\\n",
       "0   1029335.0  240891   64  female           other  2012-01-08 00:35:00  2012   \n",
       "1   2073046.0  696853   31    male           other  2012-01-08 00:41:00  2012   \n",
       "2   2073047.0  988598   19    male  road ambulance  2012-01-08 00:52:00  2012   \n",
       "3   1349154.0  941235   51    male           other  2012-01-08 01:11:00  2012   \n",
       "4   1367452.0  900875   25  female           other  2012-01-08 01:23:00  2012   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  SOB for 5/7, been to GP given prednisolone, coughing taken inhalers with minimal relief, speakin...   \n",
       "1  pt has lac down right forehead, to eyebrow, will require stitches and ADT, denies loc wound abou...   \n",
       "2                           pt expect MBA, trapped for 45mins, #right femur, had 40mg morphine, GCS 15   \n",
       "3  L) sided flank pain same as previous renal colic pain unimproved with analgesia for the past 1/5...   \n",
       "4  generalised abdo pain and associated headache for 1 year worse tonight.  Pt states that she had ...   \n",
       "\n",
       "   length  SH  SI  \\\n",
       "0     140   0   0   \n",
       "1     107   0   0   \n",
       "2      74   0   0   \n",
       "3     169   0   0   \n",
       "4     196   0   0   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  sob for 5/7 , been to gp given prednisolone , coughing taken inhalers with minimal relief , spea...  \n",
       "1  pt has lac down right forehead , to eyebrow , will require stitches and adt , denies loc wound a...  \n",
       "2             pt expect mba , trapped for 45 mins , fracture right femur , had 40 mg morphine , gcs 15  \n",
       "3  left sided flank pain same as previous renal colic pain unimproved with analgesia for the past 1...  \n",
       "4  generalised abdo pain and associated headache for 1 year worse tonight . pt states that she had ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/\" + filename + \"_cleaned.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"custom_ner\") \n",
    "def custom_ner(doc):\n",
    "    ents = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and token.text!=\"+\":\n",
    "            ents.append(Span(doc, token.i, token.i+1, label=\"ENTITY\"))\n",
    "    doc.ents = ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "@Language.component(\"bigram_detector\") \n",
    "def bigram_detector(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end) for _, start, end in matches]\n",
    "    filtered = filter_spans(spans)\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in filtered:\n",
    "            retokenizer.merge(span)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def get_canonical_name(span):\n",
    "    if span._.kb_ents:\n",
    "        concept = linker.kb.cui_to_entity[span._.kb_ents[0][0]].canonical_name.lower()\n",
    "        return re.sub(\"\\W\", \"_\", concept)\n",
    "    else:\n",
    "        return span.text\n",
    "    \n",
    "    \n",
    "def format_merged_tokens(span):\n",
    "    return re.sub(\"\\s\", \"_\", span.text)\n",
    "\n",
    "\n",
    "def apply_transformation(span, transform=\"\"):\n",
    "    if transform == \"linked\":\n",
    "        return span._.linked\n",
    "    elif transform == \"merged\":\n",
    "        return span._.merged\n",
    "    else:\n",
    "        return span.text\n",
    "    \n",
    "\n",
    "def add_negation(span, transform=\"\"):\n",
    "    return span._.negex * \"neg_\" + span._.transformed(transform)\n",
    "\n",
    "    \n",
    "def prepare_tokens(doc, negation=False, transform=\"\"):\n",
    "    if negation:\n",
    "        return \" \".join([ent._.negated(transform) for ent in doc.ents])\n",
    "    else:\n",
    "        return \" \".join([ent._.transformed(transform) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../data/most_common_bigrams.txt', 'rb') as f:\n",
    "    most_common_bigrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP pipeline: tokenizer + ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'custom_ner', 'negex']\n"
     ]
    }
   ],
   "source": [
    "# Load scispacy model\n",
    "nlp = spacy.load(\"en_core_sci_lg\", disable=['ner'])\n",
    "\n",
    "# Custom NER \n",
    "nlp.add_pipe(\"custom_ner\", last=True)\n",
    "\n",
    "# # Define bigrams\n",
    "# bigram_patterns = list(nlp.pipe(most_common_bigrams))\n",
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "# matcher.add(\"BIGRAM\", None, *bigram_patterns)\n",
    "\n",
    "# # Bigram detector\n",
    "# nlp.add_pipe(\"bigram_detector\", last=True)\n",
    "\n",
    "# # Entity linker\n",
    "# nlp.add_pipe(\"scispacy_linker\", config={'linker_name': 'mesh', 'threshold': 0.9}, last=True)\n",
    "\n",
    "# Modify negex termsets\n",
    "ts = termset('en_clinical').get_patterns()\n",
    "ts['preceding_negations'].extend([\"nil\", \"non\"])\n",
    "ts['termination'].extend([\",\", \";\", \":\", \"obviously\"])\n",
    "\n",
    "# Negation detector\n",
    "nlp.add_pipe(\"negex\", config={'ent_types': ['ENTITY']})\n",
    "\n",
    "# Set attributes\n",
    "Span.set_extension(\"linked\", getter=get_canonical_name, force=True)\n",
    "Span.set_extension(\"merged\", getter=format_merged_tokens, force=True)\n",
    "Span.set_extension(\"transformed\", method=apply_transformation, force=True)\n",
    "Span.set_extension(\"negated\", method=add_negation, force=True)\n",
    "Doc.set_extension(\"entities\", method=prepare_tokens, force=True)\n",
    "\n",
    "print(\"NLP pipeline: tokenizer + {}\".format(nlp.pipe_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53min 7s, sys: 9.66 s, total: 53min 17s\n",
      "Wall time: 53min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['doc'] = df.text_clean.apply(nlp)\n",
    "df['entities'] = df.doc.apply(lambda x: x._.entities())\n",
    "df.drop(columns='doc').to_csv(\"../data/\" + filename + \"_prepared_ents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entities and negated entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes(['bigram_detector', 'scispacy_linker']):\n",
    "    df['doc'] = df.text_clean.apply(nlp)\n",
    "\n",
    "df['entities'] = df.doc.apply(lambda x: x._.entities())\n",
    "df['neg_entities'] = df.doc.apply(lambda x: x._.entities(negation=True))\n",
    "\n",
    "df.drop(columns='doc').to_csv(\"../data/\" + filename + \"prepared_ents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merged entities and negated merged entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"EntityLinker\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['merged_entities'] = df.doc.apply(lambda x: x._.entities(transform=\"merged\"))\n",
    "df['neg_merged_entities'] = df.doc.apply(lambda x: x._.entities(negation=True, transform=\"merged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"./data/rmh_prepared_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linked entities and negated linked entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with nlp.disable_pipes([\"bigram_detector\"]):\n",
    "    df['doc'] = df.text_clean.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['linked_entities'] = df.doc.apply(lambda x: x._.entities(transform=\"linked\"))\n",
    "df['neg_linked_entities'] = df.doc.apply(lambda x: x._.entities(negation=True, transform=\"linked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"doc\").to_csv(\"./data/rmh_prepared_linked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/rmh_prepared_linked_1.csv\")\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./data/rmh_prepared_linked_2.csv\")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"./data/rmh_prepared_linked_3.csv\")\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2, df3], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/rmh_prepared_linked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/rmh_prepared_ents.csv\")\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./data/rmh_prepared_merged.csv\")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.merge(df2[[\"merged_entities\", \"neg_merged_entities\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"./data/rmh_prepared_linked.csv\")\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df3[[\"linked_entities\", \"neg_linked_entities\"]], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/rmh_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df_.loc[3, \"doc\"]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umls_entity(ent):\n",
    "    if ent._.kb_ents:\n",
    "        return linker.kb.cui_to_entity[ent._.kb_ents]\n",
    "    else:\n",
    "        return ent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df.loc[10000, 'doc'])\n",
    "print(doc, \"\\n\")\n",
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.negex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent._.kb_ents:\n",
    "        print(\"\\nEntity: \\\"{}\\\", number of linked concepts: {}\".format(ent, len(ent._.kb_ents)))\n",
    "#     print(ent.text, ent._.negex)\n",
    "#     print(canonical_name(ent), \"\\n\")\n",
    "        for concept in ent._.kb_ents:\n",
    "            print(\"\\n\", linker.kb.cui_to_entity[concept[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
