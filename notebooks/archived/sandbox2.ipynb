{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import auc, f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import nlp_utils as utils\n",
    "from nlp_utils import get_vectorizer\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# Pretty plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-ticks')\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16\n",
    "\n",
    "# Display wide columns\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of feature extraction\n",
    "vectorizer_mode = \"select features\"\n",
    "params = {'analyzer' : \"word\",\n",
    "          'ngram_range' : (1,2),\n",
    "          'use_idf' : True,\n",
    "          'mode' : \"select by pvalue\",\n",
    "          'thresh' : 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/rmh_data_prepared.csv\")\n",
    "\n",
    "class_names = (\"Controls\", \"Suicidal ideation\", \"Self harm\")\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df.y)\n",
    "\n",
    "n_controls = 10000\n",
    "df_train = pd.concat([df_train[df_train.y == 0].sample(n_controls, random_state=42), \n",
    "                      df_train[df_train.y != 0]], \n",
    "                     axis=0)\n",
    "\n",
    "print(df_train.y.value_counts())\n",
    "print(df_test.y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build-in CV with pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.y.values\n",
    "\n",
    "vectorizer = get_vectorizer(vectorizer_mode, params)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "# clf = MultinomialNB()\n",
    "\n",
    "pipe = make_pipeline(vectorizer, clf)\n",
    "\n",
    "# scores = cross_val_score(pipe, X, y, cv=10, scoring=\"f1_macro\")\n",
    "\n",
    "# print(\"Average score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "y_pred = cross_val_predict(pipe, X, y, cv=10)\n",
    "print(\"\\nPerformance evaluation:\")\n",
    "print(\"F1 score:\", f1_score(y, y_pred, average=\"macro\"))\n",
    "print(\"Classification report:\\n\", classification_report(y, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.entities.values\n",
    "y_train = df_train.SISH.values\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "X_test = df_test.entities.values\n",
    "y_test = df_test.SISH.values\n",
    "y_proba = pipe.predict_proba(X_test)\n",
    "utils.evaluate_model(y_test, y_proba, class_names, \"full testing\")\n",
    "# print(\"F1 score: %0.2f\" % f1_score(y_test, y_pred, average=\"macro\"))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_small = pd.concat([df_test[df_test.SISH == 0].sample(2500, random_state=42), \n",
    "                           df_test[df_test.SISH != 0]\n",
    "                          ], axis=0)\n",
    "\n",
    "print(df_test_small.SISH.value_counts())\n",
    "\n",
    "X_test = df_test_small.entities.values\n",
    "y_test = df_test_small.SISH.values\n",
    "y_proba = pipe.predict_proba(X_test)\n",
    "utils.evaluate_model(y_test, y_proba, class_names, \"small testing\")\n",
    "# print(\"F1 score: %0.2f\" % f1_score(y_test, y_pred, average=\"macro\"))\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ = df_test.drop(df_test[(y_test == 0) & (y_pred == 2)].index)\n",
    "X_test = df_test_.entities.values\n",
    "y_test = df_test_.SISH.values\n",
    "y_pred = pipe.predict(X_test)\n",
    "f1_score(y_test, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY?? Expects the same distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**OTHER CV IMPLEMENTATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanilla**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.SISH.values\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", \n",
    "                             stop_words=stopwords.words('english'), \n",
    "                             token_pattern=r'\\S+',\n",
    "                             ngram_range=(1,2),\n",
    "                             min_df=2, \n",
    "                             use_idf=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y, cv=10, scoring=\"f1_macro\")\n",
    "\n",
    "print(\"Average score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicit CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.SISH.values\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", \n",
    "                             stop_words=stopwords.words('english'), \n",
    "                             token_pattern=r'\\S+',\n",
    "                             ngram_range=(1,2),\n",
    "                             min_df=2, \n",
    "                             use_idf=True)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in cv.split(X_train, y):\n",
    "    clf.fit(X_train[train_index], y[train_index])\n",
    "    y_pred = clf.predict(X_train[val_index])\n",
    "    scores.append(f1_score(y[val_index], y_pred, average=\"macro\"))\n",
    "\n",
    "print(\"Average score: %0.2f (+/- %0.2f)\" % (np.asarray(scores).mean(), np.asarray(scores).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extraction inside CV loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.y.values\n",
    "\n",
    "vectorizer = get_vectorizer(vectorizer_mode, params)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in cv.split(X, y):\n",
    "    X_train = vectorizer.fit_transform(X[train_index], y[train_index])\n",
    "    clf.fit(X_train, y[train_index])\n",
    "    X_val = vectorizer.transform(X[val_index])\n",
    "    y_pred = clf.predict(X_val)\n",
    "    scores.append(f1_score(y[val_index], y_pred, average=\"macro\"))\n",
    "\n",
    "print(\"Average score: %0.4f (+/- %0.2f)\" % (np.asarray(scores).mean(), np.asarray(scores).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise vectorizer inside CV loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.y.values\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in cv.split(X, y):\n",
    "    vectorizer = get_vectorizer(vectorizer_mode, params)\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X[train_index], y[train_index])\n",
    "    clf.fit(X_train, y[train_index])\n",
    "    X_val = vectorizer.transform(X[val_index])\n",
    "    y_pred = clf.predict(X_val)\n",
    "    scores.append(f1_score(y[val_index], y_pred, average=\"macro\"))\n",
    "\n",
    "print(\"Average score: %0.4f (+/- %0.2f)\" % (np.asarray(scores).mean(), np.asarray(scores).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicit CV with pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.y.values\n",
    "\n",
    "vectorizer = get_vectorizer(vectorizer_mode, params)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "\n",
    "pipe = make_pipeline(vectorizer, clf)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in cv.split(X, y):\n",
    "    pipe.fit(X[train_index], y[train_index])\n",
    "    y_pred = pipe.predict(X[val_index])\n",
    "    scores.append(f1_score(y[val_index], y_pred, average=\"macro\"))\n",
    "\n",
    "print(\"Average score: %0.4f (+/- %0.2f)\" % (np.asarray(scores).mean(), np.asarray(scores).std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Built-in CV and feature extraction with pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.entities.values\n",
    "y = df_train.y.values\n",
    "\n",
    "vectorizer = get_vectorizer(vectorizer_mode, params)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, multi_class=\"ovr\", class_weight=\"balanced\")\n",
    "\n",
    "pipe = make_pipeline(vectorizer, clf)\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=10, scoring=\"f1_macro\")\n",
    "\n",
    "print(\"Average score: %0.4f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**CALIBRATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncalibrated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.826\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import nlp_utils as utils\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "clf = DecisionTreeClassifier()\n",
    "# pipe = make_pipeline(scaler, clf)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "scores = cross_validate(clf, X_train, y_train, n_jobs=-1, cv=10, scoring=\"roc_auc\")\n",
    "\n",
    "print('Mean ROC AUC: %.3f' % mean(scores[\"test_score\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calibrated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.826\n",
      "CPU times: user 26.9 ms, sys: 1.1 ms, total: 28 ms\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "clf = DecisionTreeClassifier()\n",
    "calibrated = CalibratedClassifierCV(clf, method='sigmoid', cv=3, ensemble=False)\n",
    "pipe = make_pipeline(scaler, calibrated)\n",
    "\n",
    "scores = cross_validate(pipe, X_train, y_train, n_jobs=-1, cv=10, scoring=\"roc_auc\")\n",
    "\n",
    "print('Mean ROC AUC: %.3f' % mean(scores[\"test_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calibration curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM reliability diagram\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[1,1], random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = SVC(probability=True)\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(testy, probs[:,1], n_bins=10, normalize=True)\n",
    "# plot perfectly calibrated\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "pyplot.plot(mpv, fop, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "                           n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "clf = SVC()\n",
    "pipe = make_pipeline(scaler, clf)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_proba = pipe.decision_function(X_test)\n",
    "# y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "\n",
    "fop, mpv = calibration_curve(y_test, y_proba, n_bins=10, normalize=True)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--');\n",
    "plt.plot(mpv, fop, marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**PROJECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=\"word\", \n",
    "                             stop_words=stopwords.words('english'), \n",
    "                             token_pattern=r'\\S+',\n",
    "                             ngram_range=(1,2),\n",
    "                             min_df=2, \n",
    "                             use_idf=True)\n",
    "                             \n",
    "X = vectorizer.fit_transform(df.entities)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "proj = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"proj1\"] = proj[:, 0]\n",
    "df[\"proj2\"] = proj[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"proj1\", y=\"proj2\", hue=\"SISH\", data=df[df.SISH != 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**TF-IDF VECTORIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(X[0, :].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, :].todense() / np.linalg.norm(X[0, :].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    norm=None,\n",
    "    use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "#     stop_words=stopwords.words('english'), \n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "#     stop_words=stopwords.words('english'), \n",
    "    min_df=2,\n",
    "#     ngram_range=(1,2),\n",
    "    norm=None,\n",
    "    use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english'), \n",
    "    min_df=2,\n",
    "    ngram_range=(1,2),\n",
    "    norm=None,\n",
    "    use_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(df[:10].entities)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[:10].y.values\n",
    "\n",
    "selector = SelectKBest(chi2, k=5)\n",
    "\n",
    "selector.fit(X, y)\n",
    "\n",
    "selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "np.asarray(feature_names)[selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = selector.transform(X)\n",
    "\n",
    "vectorizer.set_params(vocabulary=np.asarray(feature_names)[selector.get_support()])\n",
    "\n",
    "X2 = vectorizer.fit_transform(df[:10].entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, selector.get_support()].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE SELECTOR CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'word_emb' : False, \n",
    "          'model_path' : \"./models/rmh_cleaned_w2v_model.bin\",\n",
    "          'analyzer' : \"word\",\n",
    "          'ngram_range' : (1,2),\n",
    "          'use_idf' : True,\n",
    "          'select_features' : False,\n",
    "          'mode' : \"select k best\",\n",
    "          'thresh' : 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(object):\n",
    "    def __init__(self, params):\n",
    "        self.vectorizer = TfidfVectorizer(analyzer=params['analyzer'], \n",
    "                                          stop_words=stopwords.words('english'), \n",
    "                                          token_pattern=r'\\S+',\n",
    "                                          ngram_range=params['ngram_range'],\n",
    "                                          min_df=2, \n",
    "                                          use_idf=params['use_idf']\n",
    "                                         )\n",
    "        self.mode = params['mode']\n",
    "        self.thresh = params['thresh']\n",
    "        self.df_features = pd.DataFrame()\n",
    "        \n",
    "    def fit(self, X, y):    \n",
    "        X_ = self.vectorizer.fit_transform(X)\n",
    "        feature_names = self.vectorizer.get_feature_names()\n",
    "        \n",
    "        if self.mode == \"select k best\":\n",
    "            self.df_features = utils.select_k_best(X_, y, \n",
    "                                                   feature_names, \n",
    "                                                   k=self.thresh)\n",
    "        if self.mode == \"select by pvalue\":\n",
    "            self.df_features = utils.select_by_pvalue(X_, y, \n",
    "                                                      feature_names, \n",
    "                                                      alpha=self.thresh, \n",
    "                                                      verbose=False)\n",
    "                                          \n",
    "        self.vectorizer.set_params(vocabulary=self.df_features.feature.unique())\n",
    "                                          \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = self.vectorizer.transform(X)\n",
    "        return X\n",
    "                                          \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(params)\n",
    "selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[:10].y.values\n",
    "\n",
    "selector.fit(df[:10].entities, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.fit_transform(df[:10].entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
