{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9614ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import spacy\n",
    "from custom_tokenizer import combined_rule_tokenizer\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from utils import evaluate_model\n",
    "\n",
    "# Pretty plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 16\n",
    "plt.rcParams['ytick.labelsize'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2586b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_compound_token(string):\n",
    "    pattern = re.compile(\".[-/\\+_,\\?\\.].\")\n",
    "    return pattern.search(string) and string not in vocab\n",
    "\n",
    "def retokenize(doc):\n",
    "    new_doc = []\n",
    "    for token in doc:\n",
    "        if token.like_num:\n",
    "            new_doc.append(token.text)\n",
    "        elif is_compound_token(token.text):\n",
    "            [new_doc.append(new_token) for new_token in re.split('([-/\\+_,\\?\\.])', token.text)]\n",
    "        else:\n",
    "            new_doc.append(token.text)\n",
    "            \n",
    "    return ' '.join(new_doc)\n",
    "\n",
    "def spelling_correction(doc):\n",
    "    tokens = doc.split()\n",
    "    corrected_tokens = [misspelled[token][1] if token in misspelled else token for token in tokens]\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "def slang_to_generic(doc):\n",
    "    tokens = doc.split()\n",
    "    corrected_tokens = [slang_names[token] if token in slang_names else token for token in tokens]\n",
    "    return ' '.join(corrected_tokens)\n",
    "\n",
    "@Language.component(\"custom_ner\") \n",
    "def custom_ner(doc):\n",
    "    ents = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and token.text!=\"+\":\n",
    "            ents.append(Span(doc, token.i, token.i+1, label=\"CONCEPT\"))\n",
    "    doc.ents = ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a3d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "train_data = \"rmh_1217_train\"\n",
    "test_data = \"rmh_2012_2017_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b12cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.indexes.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Load pretrained classifier\u001b[39;00m\n\u001b[1;32m     66\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/pretrained_pipe_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m train_data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.sav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 67\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m thresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.372\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/pickle.py:1529\u001b[0m, in \u001b[0;36m_Unpickler.load_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1527\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1528\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1529\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(klass)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING:\n\u001b[1;32m   1579\u001b[0m         module \u001b[38;5;241m=\u001b[39m _compat_pickle\u001b[38;5;241m.\u001b[39mIMPORT_MAPPING[module]\n\u001b[0;32m-> 1580\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.indexes.numeric'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load preprocessed triage notes\n",
    "    df = pd.read_csv(\"../../data/\" + test_data + \"_normalised.csv\")\n",
    "    \n",
    "    ###TEMPORARY\n",
    "    df.rename({'preprocessed_triage_note': 'text_clean'}, axis=1, inplace=True)\n",
    "    df.text_clean.fillna(\"\", inplace=True)\n",
    "    \n",
    "    df = df[:1000]\n",
    "\n",
    "    # Load scispacy model for tokenization\n",
    "    nlp = spacy.load(\"en_core_sci_sm\", disable=['tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner'])\n",
    "    nlp.tokenizer = combined_rule_tokenizer(nlp)\n",
    "\n",
    "    # Apply tokenizer\n",
    "    df.text_clean = df.text_clean.apply(nlp)\n",
    "\n",
    "    # Load previously learned custom vocabulary (word frequency list)\n",
    "    with open (\"../../data/spelling_correction/\" + train_data + \"_vocab.txt\", 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # Initialise spellchecker with a custom vocab\n",
    "    spell = SpellChecker(language=None)\n",
    "    spell.word_frequency.load_words(vocab)\n",
    "\n",
    "    # Apply re-tokenizer\n",
    "    df.text_clean = df.text_clean.apply(retokenize)\n",
    "\n",
    "    # Define regex pattern to split leading full stop\n",
    "    pattern = re.compile(\"\\s\\.([a-z]{2,})\")\n",
    "\n",
    "    # Apply regex\n",
    "    df.text_clean = df.text_clean.apply(lambda x: pattern.sub(r\" . \\1\", x))\n",
    "\n",
    "    # Load previously learned dictionary of misspellings\n",
    "    with open (\"../../data/spelling_correction/\" + train_data + \"_misspelled_dict.txt\", 'rb') as f:\n",
    "        misspelled = pickle.load(f)\n",
    "\n",
    "    # Apply spelling correction\n",
    "    df.text_clean = df.text_clean.apply(spelling_correction)\n",
    "\n",
    "    # Load medication names\n",
    "    df_drugs = pd.read_csv(\"../../data/spelling_correction/medication_names.csv\")\n",
    "\n",
    "    df_drugs.slang = df_drugs.slang.str.strip().str.lower()\n",
    "    df_drugs.generic_name = df_drugs.generic_name.str.strip().str.lower()\n",
    "    df_drugs.dropna(subset=[\"slang\"], inplace=True)\n",
    "\n",
    "    # Create a dictionary to convert slang to generic names\n",
    "    slang_names = dict(zip(df_drugs.slang, df_drugs.generic_name))\n",
    "\n",
    "    # Apply slang replacement\n",
    "    df.text_clean = df.text_clean.apply(slang_to_generic)\n",
    "\n",
    "    # Load Scispacy model\n",
    "    nlp = spacy.load(\"en_core_sci_sm\", disable=['ner'])\n",
    "\n",
    "    # Add custom NER \n",
    "    nlp.add_pipe(\"custom_ner\", last=True)\n",
    "\n",
    "    # Apply NLP pipeline to extract concepts\n",
    "    df['doc'] = df.text_clean.apply(nlp)\n",
    "    df['concepts'] = df.doc.apply(lambda x: \" \".join([ent.text for ent in x.ents]))\n",
    "\n",
    "    # Load pretrained classifier\n",
    "    path = \"../../models/pretrained_pipe_\" + train_data + \".sav\"\n",
    "    pipe = joblib.load(path)\n",
    "    thresh = 0.372\n",
    "\n",
    "    # Make predictions\n",
    "    y_proba = pipe.predict_proba(df.concepts.fillna(\"\"))\n",
    "\n",
    "    # Convert predicted probabilities to class labels and evaluate results\n",
    "    class_names = (\"Controls\", \"Self-harm\")\n",
    "    df['y_pred'] = evaluate_model(df.SH.values, y_proba, \n",
    "                                  class_names, \"2012-2017 test\", \n",
    "                                  thresh=thresh, digits=3, \n",
    "                                  save_figures=False, filename=\"../../results/\" + test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12527d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cq/c8r6mysj28b5zqwwmdy29sx40000gq/T/ipykernel_65040/344363357.py:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  vocab = pickle.load(f)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/selfharm/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.indexes.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../models/pretrained_pipe_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m train_data \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.sav 3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.indexes.numeric'"
     ]
    }
   ],
   "source": [
    "with open (\"../../models/pretrained_pipe_\" + train_data + \".sav 3\", 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864bc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
