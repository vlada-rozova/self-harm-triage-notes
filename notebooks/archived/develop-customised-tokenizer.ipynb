{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n",
    "import time\n",
    "import utility_functions as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SISH</th>\n",
       "      <th>SI</th>\n",
       "      <th>SH</th>\n",
       "      <th>length</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>MENTAL STATE - ALTERATION IN. ETOH AFFECTED, W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>POISONING / OVERDOSE- 30MG DIAZEPAM+ETOH PHX R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>MENTAL STATE - ALTERATION IN - FOUND LYING ON ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>396</td>\n",
       "      <td>MENTAL STATE - SUICIDE ATTEMPT / RISK, PT WALK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>297</td>\n",
       "      <td>PT ACTING ABNORMALLY SINCE HAVING MISCARRIAGE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SISH  SI  SH  length                                               text\n",
       "0     1   1   0     194  MENTAL STATE - ALTERATION IN. ETOH AFFECTED, W...\n",
       "1     2   0   1     145  POISONING / OVERDOSE- 30MG DIAZEPAM+ETOH PHX R...\n",
       "2     1   1   0     269  MENTAL STATE - ALTERATION IN - FOUND LYING ON ...\n",
       "3     1   1   0     396  MENTAL STATE - SUICIDE ATTEMPT / RISK, PT WALK...\n",
       "4     0   0   0     297  PT ACTING ABNORMALLY SINCE HAVING MISCARRIAGE ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/wh_data_raw.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.find_pattern(df, \"text\", \"\\.mainta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove \"\\x7f\"\n",
    "    pattern = re.compile(r\"\\x7f\")\n",
    "    text = pattern.sub(r\" \", text)\n",
    "    # \"`/c\" to \"with\"\n",
    "    pattern = re.compile(\"`/c\")\n",
    "    text = pattern.sub(r\" \", text)\n",
    "    # Remove backslashes\n",
    "    pattern = re.compile(\"\\d\\\\\\\\\\d\")\n",
    "    text = pattern.sub(r\"/\", text)\n",
    "    pattern = re.compile(\"\\\\\\\\\")\n",
    "    text = pattern.sub(r\" \", text)\n",
    "    # Fix \"patientexpect\"\n",
    "    pattern = re.compile(r\"patientexpect\")\n",
    "    text = pattern.sub(r\" patient expect \", text)\n",
    "    \n",
    "    # \"l)\" to \"left\"\n",
    "    pattern = re.compile(\"(^|\\W)l\\)\")\n",
    "    text = pattern.sub(r\"\\1 left \", text)\n",
    "    # \"r)\" to \"right\"\n",
    "    pattern = re.compile(\"(^|\\W)r\\)\")\n",
    "    text = pattern.sub(r\"\\1 right \", text)\n",
    "    # \"@\" to \"at\"\n",
    "    pattern = re.compile(\"@\")\n",
    "    text = pattern.sub(r\" at \", text)\n",
    "    # \"#\" to \"fractured\" if not followed by number\n",
    "    pattern = re.compile(\"#(?!\\d)\")\n",
    "    text = pattern.sub(r\" fracture \", text)\n",
    "    # \"+ve\" to \"positive\"\n",
    "    pattern = re.compile(\"\\+ve(?![a-z])\")\n",
    "    text = pattern.sub(r\" positive \", text)\n",
    "    # \"-ve\" to \"positive\"\n",
    "    pattern = re.compile(\"\\-ve(?![a-z])\")\n",
    "    text = pattern.sub(r\" negative \", text)\n",
    "    # \"co operative\" and \"co-operative\" to \"cooperative\"\n",
    "    pattern = re.compile(\"co\\sop|co-op\")\n",
    "    text = pattern.sub(r\"coop\", text)\n",
    "    # \"r/ship\" to relationship\n",
    "    pattern = re.compile(\"r/ships?\")\n",
    "    text = pattern.sub(r\" relationship \", text)\n",
    "    # Remove \"+\" after digit\n",
    "    pattern = re.compile(\"(\\d)\\+\")\n",
    "    text = pattern.sub(r\"\\1 \", text)\n",
    "    \n",
    "    # Remove parentheses\n",
    "    pattern = re.compile(\"\\((.*)\\)[,\\.]?\")\n",
    "    text = pattern.sub(r\" , \\1, \", text)\n",
    "    # Remove curly brackets\n",
    "    pattern = re.compile(\"\\((.*)\\)\")\n",
    "    text = pattern.sub(r\" . \\1. \", text)\n",
    "    \n",
    "    # 1. Replace \"preg\" by \"pregnant\"\n",
    "    pattern = re.compile(\"preg$|preg\\.?(\\W)\")\n",
    "    text = pattern.sub(r\" pregnant \\1\", text)\n",
    "    \n",
    "    # 2. Replace \"reg\" by \"regular\"\n",
    "    pattern = re.compile(\"irreg$|irreg\\.?(\\W)\")\n",
    "    text = pattern.sub(r\" irregular \\1\", text)\n",
    "    pattern = re.compile(\"reg$|reg\\.?(\\W)\")\n",
    "    text = pattern.sub(r\" regular \\1\", text)\n",
    "    \n",
    "    # 3. Normalise respiratory rate\n",
    "    pattern = re.compile(\"([^a-z])rr(?![a-z])|resp\\srate|resp\\W?(?=\\d)\")\n",
    "    text = pattern.sub(r\"\\1 rr \", text)\n",
    "    \n",
    "    # 4. Normalise oxygen saturation\n",
    "    pattern = re.compile(\"sp\\s?[o0]2|sp2|spo02|sa\\s?[o0]2|sats?\\W{0,3}(?=\\d)\")\n",
    "    text = pattern.sub(r\" sao2 \", text) \n",
    "    pattern = re.compile(\"([^a-z])sp\\W{0,3}(?=[19])\")\n",
    "    text = pattern.sub(r\"\\1 sao2 \", text)\n",
    "    \n",
    "    # 5. Normilise temperature\n",
    "    pattern = re.compile(\"([^a-z])t(emp)?\\W{0,3}(?=[34]\\d)\")\n",
    "    text = pattern.sub(r\"\\1 temp \", text)\n",
    "\n",
    "    # 6. Normalise hours\n",
    "    pattern = re.compile(\"([^a-z])hrs|([^a-z])hours\")\n",
    "    text = pattern.sub(r\"\\1 hours \", text)\n",
    "     \n",
    "    # 7. Normalise heart rate\n",
    "    pattern = re.compile(\"([^a-z])hr(?![a-z])\")\n",
    "    text = pattern.sub(r\"\\1 hr \", text)\n",
    "    \n",
    "    # 8. Normalise GCS\n",
    "    pattern = re.compile(\"gsc\")\n",
    "    text = pattern.sub(r\"gcs\", text)\n",
    "    \n",
    "    # 9. Normalise on arrival\n",
    "    pattern = re.compile(\"o/a|on arrival|on assessment\")\n",
    "    text = pattern.sub(r\" o/a \", text)\n",
    "\n",
    "    # Add spaces around \"bp\"\n",
    "    pattern = re.compile(\"([^a-z])bp(?![a-z])\")\n",
    "    text = pattern.sub(r\"\\1 bp \", text)\n",
    "    \n",
    "    # Add spaces around \"bmp\", \"bsl\", \"gcs\"\n",
    "    pattern = re.compile(\"(bpm|bsl|gcs)\")\n",
    "    text = pattern.sub(r\" \\1 \", text)\n",
    "    \n",
    "    # Remove duplicated punctuation marks [-/+_,?.] and spaces\n",
    "    pattern = re.compile(\"-{2,}\")\n",
    "    text = pattern.sub(r\"-\", text)\n",
    "    pattern = re.compile(\"/{2,}\")\n",
    "    text = pattern.sub(r\"/\", text)\n",
    "    pattern = re.compile(\"\\+{2,}\")\n",
    "    text = pattern.sub(r\"+\", text)\n",
    "    pattern = re.compile(\"_{2,}\")\n",
    "    text = pattern.sub(r\"_\", text)\n",
    "    pattern = re.compile(\",{2,}\")\n",
    "    text = pattern.sub(r\",\", text)  \n",
    "    pattern = re.compile(\"\\?{2,}\")\n",
    "    text = pattern.sub(r\"?\", text)\n",
    "    pattern = re.compile(\"\\.{2,}\")\n",
    "    text = pattern.sub(r\".\", text)\n",
    "    pattern = re.compile(\"\\s{2,}\")\n",
    "    text = pattern.sub(r\" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 32.8 ms, total: 10.9 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess comments\n",
    "df['text_clean'] = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scispacy tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scispacy model for tokenization\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.7 s, sys: 496 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['tok1'] = list(nlp.pipe(df.text_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom tokenizer from the file\n",
    "from custom_tokenizer import combined_rule_tokenizer\n",
    "nlp.tokenizer = combined_rule_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, use the code below\n",
    "from typing import List\n",
    "\n",
    "from spacy.lang import char_classes\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.language import Language\n",
    "\n",
    "def remove_new_lines(text: str) -> str:\n",
    "    \"\"\"Used to preprocess away new lines in the middle of words. This function\n",
    "       is intended to be called on a raw string before it is passed through a\n",
    "       spaCy pipeline\n",
    "    @param text: a string of text to be processed\n",
    "    \"\"\"\n",
    "    text = text.replace(\"-\\n\\n\", \"\")\n",
    "    text = text.replace(\"- \\n\\n\", \"\")\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    text = text.replace(\"- \\n\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def combined_rule_prefixes() -> List[str]:\n",
    "    \"\"\"Helper function that returns the prefix pattern for the tokenizer.\n",
    "    It is a helper function to accomodate spacy tests that only test\n",
    "    prefixes.\n",
    "    \"\"\"\n",
    "    # add lookahead assertions for brackets (may not work properly for unbalanced brackets)\n",
    "    prefix_punct = char_classes.PUNCT.replace(\"|\", \" \")\n",
    "    prefix_punct = prefix_punct.replace(r\"\\(\", r\"\\((?![^\\(\\s]+\\)\\S+)\")\n",
    "    prefix_punct = prefix_punct.replace(r\"\\[\", r\"\\[(?![^\\[\\s]+\\]\\S+)\")\n",
    "    prefix_punct = prefix_punct.replace(r\"\\{\", r\"\\{(?![^\\{\\s]+\\}\\S+)\")\n",
    "\n",
    "    prefixes = (\n",
    "        [\"§\", \"%\", \"=\", r\"\\+\", \"-\", \"/\"]\n",
    "        + [\"\\.(?![0-9])\"]\n",
    "        + char_classes.split_chars(prefix_punct)\n",
    "        + char_classes.LIST_ELLIPSES\n",
    "        + char_classes.LIST_QUOTES\n",
    "        + char_classes.LIST_CURRENCY\n",
    "        + char_classes.LIST_ICONS\n",
    "    )\n",
    "    return prefixes\n",
    "\n",
    "\n",
    "def combined_rule_tokenizer(nlp: Language) -> Tokenizer:\n",
    "    \"\"\"Creates a custom tokenizer on top of spaCy's default tokenizer. The\n",
    "    intended use of this function is to replace the tokenizer in a spaCy\n",
    "    pipeline like so:\n",
    "         nlp = spacy.load(\"some_spacy_model\")\n",
    "         nlp.tokenizer = combined_rule_tokenizer(nlp)\n",
    "    @param nlp: a loaded spaCy model\n",
    "    \"\"\"\n",
    "    # remove the first hyphen to prevent tokenization of the normal hyphen\n",
    "    hyphens = char_classes.HYPHENS.replace(\"-|\", \"\", 1)\n",
    "\n",
    "    infixes = (\n",
    "        char_classes.LIST_ELLIPSES\n",
    "        + char_classes.LIST_ICONS\n",
    "        + [\n",
    "            r\"×\",  # added this special x character to tokenize it separately\n",
    "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "            r\"(?<=[{al}])\\.(?=[{au}])\".format(\n",
    "                al=char_classes.ALPHA_LOWER, au=char_classes.ALPHA_UPPER\n",
    "            ),\n",
    "            r\"(?<=[{a}]),(?=[{a}])\".format(a=char_classes.ALPHA),\n",
    "            r'(?<=[{a}])[?\";:=,.]*(?:{h})(?=[{a}])'.format(\n",
    "                a=char_classes.ALPHA, h=hyphens\n",
    "            ),\n",
    "            # removed / to prevent tokenization of /\n",
    "            r'(?<=[{a}\"])[:<>=](?=[{a}])'.format(a=char_classes.ALPHA),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prefixes = combined_rule_prefixes()\n",
    "\n",
    "    # add the last apostrophe\n",
    "    quotes = char_classes.LIST_QUOTES.copy() + [\"’\"]\n",
    "\n",
    "    # add lookbehind assertions for brackets (may not work properly for unbalanced brackets)\n",
    "    suffix_punct = char_classes.PUNCT.replace(\"|\", \" \")\n",
    "    # These lookbehinds are commented out because they are variable width lookbehinds, and as of spacy 2.1,\n",
    "    # spacy uses the re package instead of the regex package. The re package does not support variable width\n",
    "    # lookbehinds. Hacking spacy internals to allow us to use the regex package is doable, but would require\n",
    "    # creating our own instance of the language class, with our own Tokenizer class, with the from_bytes method\n",
    "    # using the regex package instead of the re package\n",
    "    # suffix_punct = suffix_punct.replace(r\"\\)\", r\"(?<!\\S+\\([^\\)\\s]+)\\)\")\n",
    "    # suffix_punct = suffix_punct.replace(r\"\\]\", r\"(?<!\\S+\\[[^\\]\\s]+)\\]\")\n",
    "    # suffix_punct = suffix_punct.replace(r\"\\}\", r\"(?<!\\S+\\{[^\\}\\s]+)\\}\")\n",
    "\n",
    "    suffixes = (\n",
    "        char_classes.split_chars(suffix_punct)\n",
    "        + char_classes.LIST_ELLIPSES\n",
    "        + quotes\n",
    "        + char_classes.LIST_ICONS\n",
    "        + [\"'s\", \"'S\", \"’s\", \"’S\", \"’s\", \"’S\"]\n",
    "        + [\"-\", \"/\", \"\\+\"]\n",
    "        + [\n",
    "            r\"(?<=[0-9])\\+\",\n",
    "            r\"(?<=°[FfCcKk])\\.\",\n",
    "            r\"(?<=[0-9])(?:{})\".format(char_classes.CURRENCY),\n",
    "            # this is another place where we used a variable width lookbehind\n",
    "            # so now things like 'H3g' will be tokenized as ['H3', 'g']\n",
    "            # previously the lookbehind was (^[0-9]+)\n",
    "            r\"(?<=[0-9])(?:{u})\".format(u=char_classes.UNITS),\n",
    "            r\"(?<=[0-9{}{}(?:{})])\\.\".format(\n",
    "                char_classes.ALPHA_LOWER, r\"%²\\-\\)\\]\\+\", \"|\".join(quotes)\n",
    "            ),\n",
    "            # add |\\d to split off the period of a sentence that ends with 1D.\n",
    "            r\"(?<=[{a}|\\d][{a}])\\.\".format(a=char_classes.ALPHA_UPPER),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "    prefix_re = compile_prefix_regex(prefixes)\n",
    "    suffix_re = compile_suffix_regex(suffixes)\n",
    "\n",
    "    tokenizer_exceptions = nlp.Defaults.tokenizer_exceptions.copy()\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "        nlp.vocab,\n",
    "        tokenizer_exceptions,\n",
    "        prefix_search=prefix_re.search,\n",
    "        suffix_search=suffix_re.search,\n",
    "        infix_finditer=infix_re.finditer,\n",
    "        token_match=nlp.tokenizer.token_match,\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 473 ms, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['tok2'] = list(nlp.pipe(df.text_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mental\n",
      "state\n",
      "-\n",
      "alteration\n",
      "in-\n",
      "{\n",
      "patient\n",
      "expect\n",
      ":\n",
      "paranoid\n",
      ",\n",
      "increasing\n",
      "thouights\n",
      "of\n",
      "being\n",
      "controlled\n",
      ",\n",
      "prev\n",
      "admissions\n",
      "to\n",
      "saapu\n",
      "with\n",
      "same\n",
      ".\n",
      "lmo\n",
      "will\n",
      "speak\n",
      "to\n",
      "emh\n",
      "directlyprev\n",
      "drug\n",
      "abuse\n",
      "}\n",
      "smoked\n",
      "ice\n",
      "tonight\n",
      ".\n",
      "a+o\n",
      ",\n",
      "nwob\n",
      "rr\n",
      "18\n",
      ",\n",
      "sao2\n",
      "99\n",
      "%\n",
      "hr\n",
      "131\n",
      ",\n",
      "very\n",
      "elevated\n",
      ",\n",
      "si\n",
      "and\n",
      "intent\n",
      "sh\n",
      "but\n",
      "nil\n",
      "plan\n",
      ",\n",
      "\"\n",
      "plan\n",
      "to\n",
      "get\n",
      "better\n",
      "\"\n",
      "good\n",
      "eye\n",
      "contact\n",
      ",\n",
      "well\n",
      "dressed\n",
      ".\n",
      "appears\n",
      "paranoid\n",
      ".\n",
      "auditry\n",
      "hallucinations\n",
      ",\n",
      "p/w/d\n",
      ",\n",
      "strong\n",
      "radial\n",
      ".\n",
      "gcs\n",
      "15\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Inspect tokens\n",
    "# i = 29681 # \"-daughter\"\n",
    "# i = 11170 # \"/risk\"\n",
    "# i = 24789 # \"351-\", \"/harm\"\n",
    "# i = 689 # \"domestic/\"\n",
    "# i = 137 # \".195\"\n",
    "# i = 4212 # \".maintaining\"\n",
    "i = 53393\n",
    "for t in df.loc[i, 'tok1']:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mental\n",
      "state\n",
      "-\n",
      "alteration\n",
      "in\n",
      "-\n",
      "{\n",
      "patient\n",
      "expect\n",
      ":\n",
      "paranoid\n",
      ",\n",
      "increasing\n",
      "thouights\n",
      "of\n",
      "being\n",
      "controlled\n",
      ",\n",
      "prev\n",
      "admissions\n",
      "to\n",
      "saapu\n",
      "with\n",
      "same\n",
      ".\n",
      "lmo\n",
      "will\n",
      "speak\n",
      "to\n",
      "emh\n",
      "directlyprev\n",
      "drug\n",
      "abuse\n",
      "}\n",
      "smoked\n",
      "ice\n",
      "tonight\n",
      ".\n",
      "a+o\n",
      ",\n",
      "nwob\n",
      "rr\n",
      "18\n",
      ",\n",
      "sao2\n",
      "99\n",
      "%\n",
      "hr\n",
      "131\n",
      ",\n",
      "very\n",
      "elevated\n",
      ",\n",
      "si\n",
      "and\n",
      "intent\n",
      "sh\n",
      "but\n",
      "nil\n",
      "plan\n",
      ",\n",
      "\"\n",
      "plan\n",
      "to\n",
      "get\n",
      "better\n",
      "\"\n",
      "good\n",
      "eye\n",
      "contact\n",
      ",\n",
      "well\n",
      "dressed\n",
      ".\n",
      "appears\n",
      "paranoid\n",
      ".\n",
      "auditry\n",
      "hallucinations\n",
      ",\n",
      "p/w/d\n",
      ",\n",
      "strong\n",
      "radial\n",
      ".\n",
      "gcs\n",
      "15\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for t in df.loc[i, 'tok2']:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    1,    20,    23,    24,    25,    29,    31,    40,    65,\n",
       "               78,\n",
       "            ...\n",
       "            53365, 53376, 53378, 53380, 53381, 53384, 53388, 53389, 53390,\n",
       "            53393],\n",
       "           dtype='int64', length=11577)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any unforeseen changes\n",
    "df.index[df.apply(lambda x: len(x.tok1) != len(x.tok2), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53394, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
